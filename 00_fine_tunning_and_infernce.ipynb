{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9589236,"sourceType":"datasetVersion","datasetId":5848331},{"sourceId":9589316,"sourceType":"datasetVersion","datasetId":5848384},{"sourceId":9589581,"sourceType":"datasetVersion","datasetId":5848581},{"sourceId":9592434,"sourceType":"datasetVersion","datasetId":5850777}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Requirements","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes\n!pip install qwen-vl-utils\n!pip install git+https://github.com/huggingface/transformers\n!pip install peft\n!pip install lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport random\nimport os\nimport gc\nimport lightning as L\nfrom tqdm import tqdm\nfrom datasets import load_dataset, load_from_disk\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BitsAndBytesConfig, Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed: int = 42) -> None:\n    # no cuda reproducibility\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\nseed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clear():\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREATE_NEW_TEST_DATASET = False\nREPO = 'sashaaadance/qwen2-vl-fine-tune'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"data = load_dataset('HuggingFaceM4/ChartQA')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['train'][0]['image']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['train'][0]['query'], data['train'][0]['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving test dataset with identifier for reproducibility","metadata":{}},{"cell_type":"code","source":"if CREATE_NEW_TEST_DATASET:\n    test_data = data['test'].add_column('id', list(range(len(data['test']))))\n    # only 250 samples\n    test_data_250 = test_data.select(\n        np.random.choice(list(range(len(test_data))), size=250)\n    )\n    test_data_250.save_to_disk('test_data_250')\n    !zip -r /kaggle/working/test_data_250.zip /kaggle/working/test_data_250","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_250 = load_from_disk('/kaggle/input/test-dataset-250/kaggle/working/test_data_250')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model quantization","metadata":{}},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    'Qwen/Qwen2-VL-2B-Instruct',\n    torch_dtype=torch.float16,\n    quantization_config=quant_config,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference before fine tunning","metadata":{}},{"cell_type":"code","source":"class DatasetQA(Dataset):\n    def __init__(self, data, processor):\n        self.data = data\n        self.processor = processor\n    \n    def __getitem__(self, item):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": self.data[item]['image']},\n\n                    {\"type\": \"text\", \"text\": ' Make your answer as neat as possible ' + self.data[item]['query']},\n                ],\n            }\n        ]\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors='pt',\n        )\n        return inputs\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained('Qwen/Qwen2-VL-2B-Instruct')\n# only 250 samples\ntest_dataset = DatasetQA(test_data_250, processor=processor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = []\nfor inputs in tqdm(test_dataset):\n    generated_ids = model.generate(**inputs.to('cuda'), max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n    outputs.append(output_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_infered = test_data_250.add_column('predictions', outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_infered[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving","metadata":{}},{"cell_type":"code","source":"test_dataset_infered.save_to_disk('test_dataset_infered')\n!zip -r /kaggle/working/test_dataset_infered.zip /kaggle/working/test_dataset_infered","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tunning","metadata":{}},{"cell_type":"code","source":"class DatasetQA_FN(Dataset):\n    def __init__(self, data, processor):\n        self.data = data\n        self.processor = processor\n    \n    def __getitem__(self, item):\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": self.data[item]['image']},\n\n                    {\"type\": \"text\", \"text\": self.data[item]['query']},\n                ],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": self.data[item]['label'][0]},\n                ],\n            }\n        ]\n        text_w_gt = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        text_wo_gt = self.processor.apply_chat_template(\n            messages[0], tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, _ = process_vision_info(messages)\n        \n        # processor is in collate function\n        input_item = {}\n        input_item['text_w_gt'] = text_w_gt\n        input_item['text_wo_gt'] = text_wo_gt\n        input_item['image'] = image_inputs[0]\n        input_item['ground_truth'] = self.data[item]['label'][0]\n        \n        return input_item\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(samples):\n    images = []\n    texts = []\n    for sample in samples:\n        images.append(sample['image'])\n        texts.append(sample['text_w_gt'])\n\n    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=128, return_tensors='pt')\n\n    labels = batch['input_ids'].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    batch['labels'] = labels\n\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    'Qwen/Qwen2-VL-2B-Instruct',\n    torch_dtype=torch.float16,\n    quantization_config=quant_config,\n)\nprocessor = AutoProcessor.from_pretrained('Qwen/Qwen2-VL-2B-Instruct')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_linear_layers(model, num_modules=-1, exclude=['lm_head']):\n    linear = torch.nn.modules.Linear\n    module_names = []\n\n    for name, module in model.named_modules():\n        if any(ex_keyword in name for ex_keyword in exclude):\n            continue\n        if isinstance(module, linear):\n            module_names.append(name)\n    \n    if num_modules > 0:\n        module_names = module_names[-num_modules:]\n\n    return module_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modules = find_linear_layers(model, num_modules=100)\nlen(modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    lora_dropout=0.1,\n    target_modules=modules,\n    init_lora_weights='gaussian',\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nclear()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QwentModule(L.LightningModule):\n    def __init__(self, config, model, train_dataset, val_dataset):\n        super().__init__()\n        self.config = config\n        self.model = model\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.batch_size = config.get('batch_size')\n\n    def training_step(self, batch, batch_idx):\n        \n        clear()\n        outputs = self.model(\n            **batch\n        )\n        loss = outputs.loss\n        \n        self.log('train_loss', loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataset_idx=0):\n        \n        clear()\n        self.model.eval()\n        outputs = self.model(\n            **batch\n        )\n        loss = outputs.loss\n\n        self.log('val_loss', loss)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get('lr'))\n\n        return optimizer\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, collate_fn=collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, collate_fn=collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = DatasetQA_FN(data['train'].select(list(range(2000))), processor)\nval_dataset = DatasetQA_FN(data['val'].select(list(range(300))), processor)\n\nconfig = {\n    'max_epochs': 10,\n    'check_val_every_n_epoch': 1,\n    'gradient_clip_val': 1.0,\n    'accumulate_grad_batches': 8,\n    'lr': 3e-4,\n    'batch_size': 1,\n    'num_nodes': 1,\n    'warmup_steps': 50,\n    'result_path': '/kaggle/working/',\n    'verbose': True,\n}\n\nmodel_module = QwentModule(config, model.to('cuda'), train_dataset, val_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks import Callback\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\nclass PushToHubCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        pl_module.model.push_to_hub(\n            REPO,\n            commit_message=f'Fine tunning, epoch {trainer.current_epoch}'\n        )\n\n    def on_train_end(self, trainer, pl_module):\n        pl_module.processor.push_to_hub(\n            REPO,\n            commit_message=f'Training done'\n        )\n        pl_module.model.push_to_hub(\n            REPO,\n            commit_message=f'Training done'\n        )\n\nearly_stop_callback = EarlyStopping(monitor='val_loss', verbose=False, mode='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clear()\ntrainer = L.Trainer(\n        accelerator='gpu',\n        devices=[0],\n        max_epochs=config.get('max_epochs'),\n        accumulate_grad_batches=config.get('accumulate_grad_batches'),\n        check_val_every_n_epoch=config.get('check_val_every_n_epoch'),\n        gradient_clip_val=config.get('gradient_clip_val'),\n        precision='16-mixed',\n        limit_val_batches=5,\n        num_sanity_val_steps=0,\n        callbacks=[PushToHubCallback(), early_stop_callback],\n)\n\ntrainer.fit(model_module)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference after tuning","metadata":{}},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    REPO,\n    torch_dtype=torch.float16,\n    quantization_config=quant_config,\n    local_files_only=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_250 = load_from_disk('/kaggle/input/test-dataset-250/kaggle/working/test_data_250')\ntest_dataset = DatasetQA(test_data_250, processor=processor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clear()\noutputs = []\nfor inputs in tqdm(test_dataset):\n    generated_ids = model.generate(**inputs.to('cuda'), max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n    outputs.append(output_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_infered = test_data_250.add_column('predictions', outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save","metadata":{}},{"cell_type":"code","source":"test_dataset_infered.save_to_disk('test_dataset_infered_after_fn')\n!zip -r /kaggle/working/test_dataset_infered_after_fn.zip /kaggle/working/test_dataset_infered_after_fn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}